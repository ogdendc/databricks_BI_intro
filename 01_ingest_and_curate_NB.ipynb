{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cef8aa3-13cf-409a-b45d-7a277e2d0ebf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## reading structured data for each customer\n",
    "> ## including indicator of historical churn ('vol_disco_ind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f70738a8-f3fb-4085-a42d-9902fa05492f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "read the structured csv data into a spark dataframe"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGF0YVBhdGggPSBmIi9Wb2x1bWVzL21haW4vb2dkZW5fZGJzcWxfaGFuZHNfb25fd29ya3Nob3AvY3N2X2luL2NodXJuX21vZGVsaW5nX2RhdGFfd19vX3RleHQuY3N2IgpzcGFya19kZiA9IHNwYXJrLnJlYWQuY3N2KGRhdGFQYXRoLCBoZWFkZXI9VHJ1ZSwgaW5mZXJTY2hlbWE9VHJ1ZSkgICMgbGV0dGluZyBTcGFyayBpbmZlciB0aGUgc2NoZW1hCmRpc3BsYXkoc3BhcmtfZGYp\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Data Profile 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 1748381440362,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": [
        [
         "mimeBundle",
         null
        ]
       ],
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "9d876ba9-4bfd-4524-a288-c5d5b23c1d2e",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 2.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1748381423559,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 1748381422748,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataPath = f\"/Volumes/main/ogden_dbsql_hands_on_workshop/csv_in/churn_modeling_data_w_o_text.csv\"\n",
    "spark_df = spark.read.csv(dataPath, header=True, inferSchema=True)  # letting Spark infer the schema\n",
    "display(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a35e797a-71f6-4c8f-8ec0-886baabaf625",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# asked the Databricks Assistant:\n",
    "\"\"\"\n",
    "write a sql query that uses the last 2 characters from the DMA field to extract the state value, and then a case when statement that maps the various states to 6 possible geographic regions such as \"southwest\". use at least 40 of the 50 states in the case when statement.\n",
    "\"\"\"\n",
    "# results may vary:\n",
    "\"\"\"\n",
    "SELECT \n",
    "    *,\n",
    "    SUBSTRING(DMA, -2) AS state,\n",
    "    CASE \n",
    "        WHEN SUBSTRING(DMA, -2) IN ('TX', 'NM', 'AZ', 'OK') THEN 'southwest'\n",
    "        WHEN SUBSTRING(DMA, -2) IN ('CA', 'NV', 'UT', 'CO') THEN 'west'\n",
    "        WHEN SUBSTRING(DMA, -2) IN ('NY', 'NJ', 'PA', 'CT', 'MA', 'VT', 'NH', 'ME', 'RI') THEN 'northeast'\n",
    "        WHEN SUBSTRING(DMA, -2) IN ('FL', 'GA', 'AL', 'SC', 'NC', 'VA', 'WV', 'KY', 'TN', 'MS', 'AR', 'LA') THEN 'southeast'\n",
    "        WHEN SUBSTRING(DMA, -2) IN ('IL', 'IN', 'OH', 'MI', 'WI', 'MN', 'IA', 'MO', 'ND', 'SD', 'NE', 'KS') THEN 'midwest'\n",
    "        WHEN SUBSTRING(DMA, -2) IN ('WA', 'OR', 'ID', 'MT', 'WY', 'AK', 'HI') THEN 'northwest'\n",
    "        ELSE 'other'\n",
    "    END AS region\n",
    "FROM \n",
    "    your_table_name\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "056fe8d8-64e8-4f57-82f2-7547efff4f7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# asked the Databricks Assistant:\n",
    "\"\"\"\n",
    "rewrite the above to use pyspark code\n",
    "\"\"\"\n",
    "# results may vary:\n",
    "\"\"\"\n",
    "%python\n",
    "from pyspark.sql.functions import substring, when, col\n",
    "\n",
    "# Extract the state value from the last 2 characters of the DMA field\n",
    "spark_df = spark_df.withColumn(\"state\", substring(col(\"DMA\"), -2, 2))\n",
    "\n",
    "# Map the states to geographic regions\n",
    "spark_df = spark_df.withColumn(\n",
    "    \"region\",\n",
    "    when(col(\"state\").isin(\"TX\", \"NM\", \"AZ\", \"OK\"), \"southwest\")\n",
    "    .when(col(\"state\").isin(\"CA\", \"NV\", \"UT\", \"CO\"), \"west\")\n",
    "    .when(col(\"state\").isin(\"NY\", \"NJ\", \"PA\", \"CT\", \"MA\", \"VT\", \"NH\", \"ME\", \"RI\"), \"northeast\")\n",
    "    .when(col(\"state\").isin(\"FL\", \"GA\", \"AL\", \"SC\", \"NC\", \"VA\", \"WV\", \"KY\", \"TN\", \"MS\", \"AR\", \"LA\"), \"southeast\")\n",
    "    .when(col(\"state\").isin(\"IL\", \"IN\", \"OH\", \"MI\", \"WI\", \"MN\", \"IA\", \"MO\", \"ND\", \"SD\", \"NE\", \"KS\"), \"midwest\")\n",
    "    .when(col(\"state\").isin(\"WA\", \"OR\", \"ID\", \"MT\", \"WY\", \"AK\", \"HI\"), \"northwest\")\n",
    "    .otherwise(\"other\")\n",
    ")\n",
    "\n",
    "display(spark_df)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11534102-8253-4e3d-9e70-f8321deaa14c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "using PySpark code from the Assistant to add 'region'"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring, when, col\n",
    "\n",
    "# Extract the state value from the last 2 characters of the DMA field\n",
    "spark_df = spark_df.withColumn(\"state\", substring(col(\"DMA\"), -2, 2))\n",
    "\n",
    "# Map the states to geographic regions\n",
    "spark_df = spark_df.withColumn(\n",
    "    \"region\",\n",
    "    when(col(\"state\").isin(\"TX\", \"NM\", \"AZ\", \"OK\"), \"southwest\")\n",
    "    .when(col(\"state\").isin(\"CA\", \"NV\", \"UT\", \"CO\"), \"west\")\n",
    "    .when(col(\"state\").isin(\"NY\", \"NJ\", \"PA\", \"CT\", \"MA\", \"VT\", \"NH\", \"ME\", \"RI\"), \"northeast\")\n",
    "    .when(col(\"state\").isin(\"FL\", \"GA\", \"AL\", \"SC\", \"NC\", \"VA\", \"WV\", \"KY\", \"TN\", \"MS\", \"AR\", \"LA\"), \"southeast\")\n",
    "    .when(col(\"state\").isin(\"IL\", \"IN\", \"OH\", \"MI\", \"WI\", \"MN\", \"IA\", \"MO\", \"ND\", \"SD\", \"NE\", \"KS\"), \"midwest\")\n",
    "    .when(col(\"state\").isin(\"WA\", \"OR\", \"ID\", \"MT\", \"WY\", \"AK\", \"HI\"), \"northwest\")\n",
    "    .otherwise(\"other\")\n",
    ")\n",
    "\n",
    "display(spark_df[\"cust_num\", \"DMA\", \"region\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d028bf36-2ec5-4bb9-9c97-61500c85bb1e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "creating a temporary view to enable SQL against the data"
    }
   },
   "outputs": [],
   "source": [
    "# API to present Spark dataframes as a queriable views, so we can use spark.sql() or run %sql cells:\n",
    "\n",
    "spark_df.createOrReplaceTempView(\"spark_df_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41b3299f-99fa-4075-8a11-3fe2cd3d0732",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT region, \n",
    "       COUNT(*)                                      AS customer_cnt,\n",
    "       SUM(vol_disco_ind)                            AS churn_cnt,\n",
    "       format_number(churn_cnt/customer_cnt, '0.0%') AS churn_rate\n",
    "FROM spark_df_view\n",
    "GROUP BY region\n",
    "ORDER BY customer_cnt DESC\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cd41e1e-7f82-488e-a763-5605fb545bc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## reading call center rep notes for each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc7f6f67-ca97-4cc2-a914-e850234be833",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "read the UNstructured csv data into a spark dataframe"
    }
   },
   "outputs": [],
   "source": [
    "dataPath = f\"/Volumes/main/ogden_dbsql_hands_on_workshop/csv_in/text_encrypted.csv\"\n",
    "spark_txt_df = spark.read.csv(dataPath, header=True, inferSchema=True)  # letting Spark infer the schema\n",
    "display(spark_txt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed26314e-459c-43fe-a67e-88e6f2d1bbc2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "creating a temporary view to enable SQL against the data"
    }
   },
   "outputs": [],
   "source": [
    "# queriable view for the UNstructured data\n",
    "\n",
    "spark_txt_df.createOrReplaceTempView(\"spark_txt_df_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5715e838-b7f8-427f-977c-1036ead6082c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## saving joined data as output table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94a1c4b3-a7d3-4451-ad59-6efdb26f2fce",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "creating a 'silver' output table for analysis, modeling, etc"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE main.ogden_dbsql_hands_on_workshop.churn_silver\n",
    "AS SELECT a.*,\n",
    "          b.text_string\n",
    "FROM spark_df_view a\n",
    "LEFT JOIN spark_txt_df_view b\n",
    "ON a.cust_num = b.cust_num\n",
    ";\n",
    "\n",
    "SELECT * FROM main.ogden_dbsql_hands_on_workshop.churn_silver;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 578676876744258,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "01_ingest_and_curate_NB",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
